{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Kb_QiR7pTCgN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_log=pd.read_csv('/content/logs_preprocessed.csv')\n",
        "print(df_log.columns)\n",
        "print(df_log.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3gR-Yf9T2Ka",
        "outputId": "67be1e13-da0e-4c9e-ad03-5018af5c3667"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['filename', 'dut', 'dut_version', 'os_version', 'config',\n",
            "       'test_case_id', 'line_number', 'timestamp', 'run_date', 'status',\n",
            "       'error_msg', 'suite', 'raw_line'],\n",
            "      dtype='object')\n",
            "(8605, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_cluster=pd.read_csv('/content/failure_patterns_labeled_human.csv')\n",
        "print(df_cluster.columns)\n",
        "print(df_cluster.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqDDSkaqUF5x",
        "outputId": "65ecdc74-895a-4c32-988c-2a0fb9be0303"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['filename', 'dut', 'dut_version', 'os_version', 'config',\n",
            "       'test_case_id', 'line_number', 'timestamp', 'run_date', 'status',\n",
            "       'error_msg', 'suite', 'raw_line', 'failure_freq_suite',\n",
            "       'failure_freq_dut', 'execution_duration', 'time_since_last_failure',\n",
            "       'recent_failure_flag', 'config_hash', 'cluster', 'root_cause_label',\n",
            "       'keywords'],\n",
            "      dtype='object')\n",
            "(8605, 22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_classified=pd.read_csv('/content/classified_logs.csv')\n",
        "print(df_classified.columns)\n",
        "print(df_classified.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMu6EXLUUo8Z",
        "outputId": "3c003a67-6a49-450e-b647-d9ee440f7e37"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['TestCase', 'DUT', 'Suite', 'PredictedLabel', 'ActualStatus',\n",
            "       'Confidence', 'LowConfidenceFlag', 'ErrorMsg'],\n",
            "      dtype='object')\n",
            "(1721, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_anomaly = pd.read_csv('/content/anomaly_full_scores.csv')\n",
        "print(df_anomaly.columns)\n",
        "print(df_anomaly.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spi4ypncVHIZ",
        "outputId": "f84fb2a1-566e-402f-eefa-2755df421e7a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['filename', 'dut', 'dut_version', 'os_version', 'config',\n",
            "       'test_case_id', 'line_number', 'timestamp', 'run_date', 'status',\n",
            "       'error_msg', 'suite', 'raw_line', 'failure_freq_suite',\n",
            "       'failure_freq_dut', 'execution_duration', 'time_since_last_failure',\n",
            "       'recent_failure_flag', 'config_hash', 'cluster', 'root_cause_label',\n",
            "       'keywords', 'anomaly_score', 'is_anomaly', 'LogID', 'MessageSnippet'],\n",
            "      dtype='object')\n",
            "(8605, 26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Load all CSVs\n",
        "# ---------------------------------------------------------\n",
        "df_logs       = pd.read_csv('/content/logs_preprocessed.csv')\n",
        "df_cluster    = pd.read_csv('/content/failure_patterns_labeled_human.csv')\n",
        "df_classified = pd.read_csv('/content/classified_logs.csv')\n",
        "df_anomaly    = pd.read_csv('/content/anomaly_full_scores.csv')\n",
        "\n",
        "# Standardize column names: lowercase and strip\n",
        "# ---------------------------------------------------------\n",
        "for df_tmp in [df_logs, df_cluster, df_classified, df_anomaly]:\n",
        "    df_tmp.columns = df_tmp.columns.str.strip().str.lower()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E8r-N-WwVQEh"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename df_classified columns to match df_logs\n",
        "# ---------------------------------------------------------\n",
        "df_classified = df_classified.rename(columns={\n",
        "    'testcase': 'test_case_id',\n",
        "    'dut': 'dut',\n",
        "    'suite': 'suite'\n",
        "})"
      ],
      "metadata": {
        "id": "Miv2sw27V4Sp"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge logs + classified\n",
        "# Use unique identifier columns to avoid Cartesian product\n",
        "# ---------------------------------------------------------\n",
        "# Check for uniqueness\n",
        "print(\"Rows before merge:\", len(df_logs))\n",
        "merge_keys_clf = ['test_case_id','suite']  # adjust if more unique identifiers exist\n",
        "\n",
        "df_classified_unique = df_classified.drop_duplicates(subset=merge_keys_clf)\n",
        "\n",
        "df = df_logs.merge(df_classified_unique,\n",
        "                   on=merge_keys_clf,\n",
        "                   how='left',\n",
        "                   suffixes=('', '_clf'),\n",
        "                   validate='many_to_one')  # many logs -> 1 classified row"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHbyJgHSWFhg",
        "outputId": "8b9aab29-e1cd-4e93-f213-7bc904f387f6"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows before merge: 8605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge with cluster\n",
        "# ---------------------------------------------------------\n",
        "merge_keys_clu = ['test_case_id','suite']\n",
        "df_cluster_unique = df_cluster.drop_duplicates(subset=merge_keys_clu)\n",
        "\n",
        "df = df.merge(df_cluster_unique,\n",
        "              on=merge_keys_clu,\n",
        "              how='left',\n",
        "              suffixes=('', '_clu'),\n",
        "              validate='many_to_one')"
      ],
      "metadata": {
        "id": "JvKLhXtHW1Ug"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check columns in anomaly\n",
        "print(df_anomaly.columns)\n",
        "\n",
        "# Rename to match df_logs\n",
        "df_anomaly = df_anomaly.rename(columns={'testcase': 'test_case_id', 'suite': 'suite', 'filename': 'filename'})\n",
        "\n",
        "merge_keys_anom = [c for c in ['filename','test_case_id','suite'] if c in df.columns and c in df_anomaly.columns]\n",
        "\n",
        "if merge_keys_anom:\n",
        "    df = df.merge(df_anomaly[merge_keys_anom + ['anomaly_score','is_anomaly','messagesnippet']],\n",
        "                  on=merge_keys_anom,\n",
        "                  how='left')\n",
        "else:\n",
        "    print(\"Skipping anomaly merge: no common columns found\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oHgYSi4W5ZI",
        "outputId": "fd844c69-0ceb-447f-99f3-569812dfada9"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['filename', 'dut', 'dut_version', 'os_version', 'config',\n",
            "       'test_case_id', 'line_number', 'timestamp', 'run_date', 'status',\n",
            "       'error_msg', 'suite', 'raw_line', 'failure_freq_suite',\n",
            "       'failure_freq_dut', 'execution_duration', 'time_since_last_failure',\n",
            "       'recent_failure_flag', 'config_hash', 'cluster', 'root_cause_label',\n",
            "       'keywords', 'anomaly_score', 'is_anomaly', 'logid', 'messagesnippet'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert timestamp to datetime\n",
        "# ---------------------------------------------------------\n",
        "df['executiontime'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Fill missing anomaly values\n",
        "# ---------------------------------------------------------\n",
        "df['anomaly_score'] = df.get('anomaly_score', 0).fillna(0)\n",
        "df['is_anomaly'] = df.get('is_anomaly', 0).fillna(0).astype(int)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. Test case features\n",
        "# ---------------------------------------------------------\n",
        "df['is_fail'] = df['status'].apply(lambda x: 1 if str(x).lower() == \"fail\" else 0)\n",
        "df['past_failure_rate'] = df.groupby('test_case_id')['is_fail']\\\n",
        "                            .transform(lambda x: x.shift().expanding().mean()).fillna(0)\n",
        "\n",
        "df['last_fail_time'] = df[df['is_fail']==1].groupby('test_case_id')['executiontime'].transform('last')\n",
        "df['time_since_last_failure'] = (df['executiontime'] - df['last_fail_time']).dt.total_seconds().fillna(0)\n",
        "df['execution_frequency'] = df.groupby('test_case_id').cumcount()\n",
        "\n",
        "if 'execution_duration' in df.columns:\n",
        "    df['avg_duration'] = df.groupby('test_case_id')['execution_duration'].transform(lambda x: x.expanding().mean())\n",
        "else:\n",
        "    df['avg_duration'] = 0\n"
      ],
      "metadata": {
        "id": "9U8ijEkkW8Rv"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Keyword features\n",
        "# ---------------------------------------------------------\n",
        "df['messagesnippet'] = df.get('messagesnippet', \"\").fillna(\"\")\n",
        "df['keyword_fail'] = df['messagesnippet'].str.contains(r\"fail|error|exception|abort\", case=False, regex=True).astype(int)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. Failure cluster\n",
        "# ---------------------------------------------------------\n",
        "df['clusterlabel'] = df['cluster'].fillna(-1) if 'cluster' in df.columns else -1\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4. Anomaly features\n",
        "# ---------------------------------------------------------\n",
        "df['rolling_anomaly_rate'] = df.groupby('test_case_id')['is_anomaly']\\\n",
        "                               .transform(lambda x: x.shift().expanding().mean()).fillna(0)\n",
        "df['mean_anomaly_score'] = df.groupby('test_case_id')['anomaly_score'].transform(lambda x: x.expanding().mean())\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 5. Environment / DUT features\n",
        "# ---------------------------------------------------------\n",
        "env_cols = ['dut_version','dut','config','regression_group','schedule_cycle']\n",
        "encoder = LabelEncoder()\n",
        "for col in env_cols:\n",
        "    if col in df.columns:\n",
        "        df[col] = df[col].fillna('Unknown')\n",
        "        df[col] = encoder.fit_transform(df[col].astype(str))\n",
        "\n"
      ],
      "metadata": {
        "id": "_ETOQi2gXAaP"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Metadata features\n",
        "# ---------------------------------------------------------\n",
        "df['exec_day'] = df['executiontime'].dt.dayofweek\n",
        "df['exec_hour'] = df['executiontime'].dt.hour\n",
        "\n",
        "for col in ['buildversion','suite']:\n",
        "    if col in df.columns:\n",
        "        df[col] = df[col].fillna('Unknown')\n",
        "        df[col] = encoder.fit_transform(df[col].astype(str))\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 7. Target encoding\n",
        "# ---------------------------------------------------------\n",
        "df['target'] = df['status'].apply(lambda x: 1 if str(x).lower() == \"fail\" else 0)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Select final ML feature columns\n",
        "# ---------------------------------------------------------\n",
        "feature_cols = [\n",
        "    'test_case_id','executiontime',\n",
        "    'past_failure_rate','time_since_last_failure','execution_frequency','avg_duration',\n",
        "    'keyword_fail','clusterlabel','rolling_anomaly_rate','mean_anomaly_score',\n",
        "    'exec_day','exec_hour','dut_version','dut','config',\n",
        "    'regression_group','schedule_cycle','buildversion','suite',\n",
        "    'target'\n",
        "]\n",
        "feature_cols = [c for c in feature_cols if c in df.columns]\n",
        "df_features = df[feature_cols]\n",
        "# 1. Test case features\n",
        "# ---------------------------------------------------------\n",
        "df['is_fail'] = df['status'].apply(lambda x: 1 if str(x).lower() == \"fail\" else 0)\n",
        "df['past_failure_rate'] = df.groupby('test_case_id')['is_fail']\\\n",
        "                            .transform(lambda x: x.shift().expanding().mean()).fillna(0)\n",
        "\n",
        "df['last_fail_time'] = df[df['is_fail']==1].groupby('test_case_id')['executiontime'].transform('last')\n",
        "df['time_since_last_failure'] = (df['executiontime'] - df['last_fail_time']).dt.total_seconds().fillna(0)\n",
        "df['execution_frequency'] = df.groupby('test_case_id').cumcount()\n",
        "\n",
        "if 'execution_duration' in df.columns:\n",
        "    df['avg_duration'] = df.groupby('test_case_id')['execution_duration'].transform(lambda x: x.expanding().mean())\n",
        "else:\n",
        "    df['avg_duration'] = 0"
      ],
      "metadata": {
        "id": "uSshSCtqYQIW"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save CSV\n",
        "# ---------------------------------------------------------\n",
        "df_features.to_csv('/content/feature_engineered_testcases.csv', index=False)\n",
        "print(\"Feature Engineering Completed ✔\")\n",
        "print(\"Saved: feature_engineered_testcases.csv\")\n",
        "print(\"Final row count:\", len(df_features))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RG-B8gSYS3e",
        "outputId": "18370a0a-a786-4acb-e30d-e2a664acd756"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Engineering Completed ✔\n",
            "Saved: feature_engineered_testcases.csv\n",
            "Final row count: 22137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7b__x1YrYmSm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}